{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n<p style=\"background-color:#e6f7ff; \n          padding:15px; \n          color:#111;\n          font-size:16px;\n          border-width:3px; \n          border-color:#d0eefc; \n          border-style:solid;\n          border-radius:6px\"> üîç This project focus on fine-tuning an existing LLM for enhanced dialogue summarization, we'll use the <a style=\"text-decoration: underline;\" href=\"https://huggingface.co/docs/transformers/model_doc/flan-t5\"><code>FLAN-T5</code></a> model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, we'll explore a full fine-tuning approach and evaluate the results with <code>ROUGE</code> metrics, then perform Parameter Efficient Fine-Tuning <code>(PEFT)</code>, evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics.\n</p>\n\n","metadata":{}},{"cell_type":"markdown","source":"$$$$","metadata":{}},{"cell_type":"markdown","source":"### Setup","metadata":{}},{"cell_type":"code","source":"!pip install --quiet evaluate\n!pip install --quiet rouge_score\n!pip install --quiet peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:32:57.790210Z","iopub.execute_input":"2024-11-02T17:32:57.790559Z","iopub.status.idle":"2024-11-02T17:33:11.023176Z","shell.execute_reply.started":"2024-11-02T17:32:57.790522Z","shell.execute_reply":"2024-11-02T17:33:11.021971Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-02T18:37:17.800423Z","iopub.execute_input":"2024-11-02T18:37:17.801170Z","iopub.status.idle":"2024-11-02T18:37:17.808612Z","shell.execute_reply.started":"2024-11-02T18:37:17.801127Z","shell.execute_reply":"2024-11-02T18:37:17.807730Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":41},{"cell_type":"markdown","source":"## Load Dataset and LLM","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"knkarthick/dialogsum\")\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:33:32.500992Z","iopub.execute_input":"2024-11-02T17:33:32.501611Z","iopub.status.idle":"2024-11-02T17:33:35.316170Z","shell.execute_reply.started":"2024-11-02T17:33:32.501575Z","shell.execute_reply":"2024-11-02T17:33:35.315226Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093da0694780446195c84fd4b292a7d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/11.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9745e85d943448d917d8ad43a0c1b4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/442k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf585f9994ad48ddbe3c1c9ee8d6f0b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3c42e26b0d4ae296f05a15d83d47d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51edb0af52144d2dbdffb8ee8ce480a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d434ed467c483ba24d3fc44338e654"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb21001dd0204b99be1e1aadfdf4c273"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"**Inspect an example from the dataset**","metadata":{}},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:33:35.318615Z","iopub.execute_input":"2024-11-02T17:33:35.319499Z","iopub.status.idle":"2024-11-02T17:33:35.328647Z","shell.execute_reply.started":"2024-11-02T17:33:35.319463Z","shell.execute_reply":"2024-11-02T17:33:35.327754Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"**Load the pre-trained FLAN-T5 model and its tokenizer**","metadata":{}},{"cell_type":"code","source":"model_name='google/flan-t5-base'\n\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n                                torch_dtype=torch.bfloat16,\n                                )\noriginal_model = original_model.to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**N¬∞ of model parameters,trainable parameters**","metadata":{}},{"cell_type":"code","source":"def number_of_trainable_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    print(f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:35:48.148072Z","iopub.execute_input":"2024-11-02T17:35:48.148996Z","iopub.status.idle":"2024-11-02T17:35:48.155826Z","shell.execute_reply.started":"2024-11-02T17:35:48.148942Z","shell.execute_reply":"2024-11-02T17:35:48.154421Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"number_of_trainable_parameters(original_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:35:49.221045Z","iopub.execute_input":"2024-11-02T17:35:49.221452Z","iopub.status.idle":"2024-11-02T17:35:49.228843Z","shell.execute_reply.started":"2024-11-02T17:35:49.221408Z","shell.execute_reply":"2024-11-02T17:35:49.227866Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 247577856\nall model parameters: 247577856\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"$$$$","metadata":{}},{"cell_type":"markdown","source":"## Zero-shot Inference","metadata":{}},{"cell_type":"markdown","source":"<div style=\"\n    background-color: #fff6ff;\n    color: #111;\n    font-size: 16px;\n    padding: 15px;\n    border-width: 3px;\n    border-color: #efe6ef;\n    border-style: solid;\n    border-radius: 6px;\n            \">\n  üìö The <code>Zero-shot</code> approach allows us to generate summaries without needing additional fine-tuning on the specific dataset, relying instead on its pre-trained capabilities.</br>\nThe process involves creating a structured prompt that includes the dialogue text.\n</div>","metadata":{}},{"cell_type":"code","source":"index = 200\n\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:35:55.905525Z","iopub.execute_input":"2024-11-02T17:35:55.905934Z","iopub.status.idle":"2024-11-02T17:35:55.911496Z","shell.execute_reply.started":"2024-11-02T17:35:55.905886Z","shell.execute_reply":"2024-11-02T17:35:55.910613Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\ngen_summ = original_model.generate(\n        inputs[\"input_ids\"].to(device), \n        max_new_tokens=200,\n    )[0] \n\noutput = tokenizer.decode(\n    gen_summ,\n    skip_special_tokens=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:36:45.649540Z","iopub.execute_input":"2024-11-02T17:36:45.650270Z","iopub.status.idle":"2024-11-02T17:36:46.650316Z","shell.execute_reply.started":"2024-11-02T17:36:45.650232Z","shell.execute_reply":"2024-11-02T17:36:46.649503Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"dash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:36:50.053018Z","iopub.execute_input":"2024-11-02T17:36:50.053406Z","iopub.status.idle":"2024-11-02T17:36:50.059287Z","shell.execute_reply.started":"2024-11-02T17:36:50.053367Z","shell.execute_reply":"2024-11-02T17:36:50.058412Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n#Person1#: Have you considered upgrading your system?\n#Person2#: Yes, but I'm not sure what exactly I would need.\n#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n#Person2#: That would be a definite bonus.\n#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n#Person2#: How can we do that?\n#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n#Person2#: No.\n#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n#Person2#: That sounds great. Thanks.\n\nSummary:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n#Person1#: I'm thinking of upgrading my computer.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"$$$$","metadata":{}},{"cell_type":"markdown","source":"## Full fine-tuning","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#ffe6f7; \n          padding:15px; \n          color:#111;\n          font-size:16px;\n          border-width:3px; \n          border-color:#f5dce9; \n          border-style:solid;\n          border-radius:6px\"> üìÑ In this section we'll finetune the model using <code>full-precision</code>, we need to convert the dialog-summary pairs into explicit instructions for the LLM.</br>\n          We can use a simple instruction prompt to prepend the dialog with <code>Summarize the following conversation</code> and start the summary with <code>Summary</code> as follows:\n</br></br>\n    \n<em>Training prompt (dialogue):</em>\n<p style=\"\n    background-color: #555;\n    color: #fff;\n    font-size: 16px;\n    padding: 10px;\n    border-width: 2px;\n    border-color: #111;\n    border-style: solid;\n    display: inline-block;\n    border-radius: 6px;\"\n        > Summarize the following conversation:</br>\n         &nbsp; &nbsp; &nbsp; Chris: This is his part of the conversation.</br>\n         &nbsp; &nbsp; &nbsp; Antje: This is her part of the conversation.</br>\n    Summary: </br>\n</p>\n    </br>\n\n<em>Training response (summary):</em>\n<p style=\"\n    background-color: #555;\n    color: #fff;\n    font-size: 16px;\n    padding: 10px;\n    border-width: 2px;\n    border-color: #111;\n    border-style: solid;\n    display: inline-block;\n    border-radius: 6px;\"\n        >    Both Chris and Antje participated in the    \n</p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"**Tokenize Dataset**","metadata":{}},{"cell_type":"code","source":"def tokenize_function(example):\n    start_prompt = 'Summarize the following conversation:\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n    \n    example['input_ids'] = tokenizer(prompt,\n                                     padding=\"max_length\",\n                                     truncation=True,\n                                     return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"summary\"],\n                                  padding=\"max_length\",\n                                  truncation=True,\n                                  return_tensors=\"pt\").input_ids\n    \n    return example\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:38:05.910120Z","iopub.execute_input":"2024-11-02T17:38:05.910845Z","iopub.status.idle":"2024-11-02T17:38:21.051219Z","shell.execute_reply.started":"2024-11-02T17:38:05.910804Z","shell.execute_reply":"2024-11-02T17:38:21.050435Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12460 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c208830fdab4a958214788c07ca61ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f78005c6fc5748caab04e57a8500ab0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41128b1bfbcf40049c40735128d9b1a6"}},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"**Shapes of the dataset**","metadata":{}},{"cell_type":"code","source":"print(f\"Shapes of the datasets:\")\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:38:21.081276Z","iopub.execute_input":"2024-11-02T17:38:21.081576Z","iopub.status.idle":"2024-11-02T17:38:21.087460Z","shell.execute_reply.started":"2024-11-02T17:38:21.081542Z","shell.execute_reply":"2024-11-02T17:38:21.086381Z"}},"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (12460, 2)\nValidation: (500, 2)\nTest: (1500, 2)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### Fine-tune model with preprocessed Dataset","metadata":{}},{"cell_type":"code","source":"output_dir = f'./dialogue-summary-training'\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-5,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    logging_steps=80,\n    auto_find_batch_size=True,\n    report_to='none'\n)\n\ntrainer = Trainer(\n    model=original_model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T17:50:34.330726Z","iopub.execute_input":"2024-11-02T17:50:34.331443Z","iopub.status.idle":"2024-11-02T17:50:34.372995Z","shell.execute_reply.started":"2024-11-02T17:50:34.331399Z","shell.execute_reply":"2024-11-02T17:50:34.372194Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Free up GPU memory**","metadata":{}},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"instruct_model = AutoModelForSeq2SeqLM.from_pretrained(f'./{output_dir}/checkpoint-1558',\n                                                       torch_dtype=torch.bfloat16)\ninstruct_model = instruct_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T18:31:56.989572Z","iopub.execute_input":"2024-11-02T18:31:56.989990Z","iopub.status.idle":"2024-11-02T18:31:58.996639Z","shell.execute_reply.started":"2024-11-02T18:31:56.989951Z","shell.execute_reply":"2024-11-02T18:31:58.995837Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"\n### Evaluate the Model Qualitatively (Human Evaluationel)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"\n    background-color: #e6ffe6; \n    color: #111;\n    font-size: 16px;\n    padding: 15px;\n    border-width: 3px;\n    border-color: #d9f5d9;\n    border-style: solid;\n    border-radius: 6px;\n            \">\n  üìä As with many GenAI applications, a <code>qualitative</code> approach is usually a good starting point. In the example below (the same one we started this notebook with).<br> We see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model.\n</div>\n\n","metadata":{}},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nhuman_baseline_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T18:34:20.453628Z","iopub.execute_input":"2024-11-02T18:34:20.454493Z","iopub.status.idle":"2024-11-02T18:34:20.460858Z","shell.execute_reply.started":"2024-11-02T18:34:20.454439Z","shell.execute_reply":"2024-11-02T18:34:20.459691Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\ninput_ids = input_ids.to(device)\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids,\n                               generation_config=GenerationConfig(max_new_tokens=200,\n                                                                    num_beams=1)\n                                                )\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0],\n                                              skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids,\n                                generation_config=GenerationConfig(max_new_tokens=200,\n                                                                   num_beams=1)\n                                                )\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0],\n                                              skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T18:33:26.429922Z","iopub.execute_input":"2024-11-02T18:33:26.430293Z","iopub.status.idle":"2024-11-02T18:33:28.231713Z","shell.execute_reply.started":"2024-11-02T18:33:26.430260Z","shell.execute_reply":"2024-11-02T18:33:28.230930Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"print(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T18:33:32.119418Z","iopub.execute_input":"2024-11-02T18:33:32.120055Z","iopub.status.idle":"2024-11-02T18:33:32.125726Z","shell.execute_reply.started":"2024-11-02T18:33:32.120012Z","shell.execute_reply":"2024-11-02T18:33:32.124743Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\n#Person1#: I'm not sure what you mean. #Person2#: Well, I'm not sure what you mean. I'm not sure what I'm doing wrong. I'm not sure what I'm doing wrong. I'm not sure what I'm doing wrong.\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\nYou might want to upgrade your computer.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"\n### Evaluate the Model Quantitatively (with ROUGE Metricng)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"\n    background-color: #fff6e4; \n    color: #111;\n    font-size: 16px;\n    padding: 15px;\n    border-width: 3px;\n    border-color: #f5ecda;\n    border-style: solid;\n    border-radius: 6px;\n            \">\n  ü§ñ The <code>ROUGE</code> metric helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.\n</div>\n\n\n","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T18:35:48.105053Z","iopub.execute_input":"2024-11-02T18:35:48.106181Z","iopub.status.idle":"2024-11-02T18:35:48.589515Z","shell.execute_reply.started":"2024-11-02T18:35:48.106130Z","shell.execute_reply":"2024-11-02T18:35:48.588519Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"**Generate summaries with both original and Instruct models**","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test']['dialogue']\nhuman_baseline_summaries = dataset['test']['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\n\nfor dialogue in tqdm(dialogues,desc='dialogues'):\n    prompt = f\"\"\"\n    Summarize the following conversation.\n\n    {dialogue}\n\n    Summary: \"\"\"\n    input_ids = tokenizer(prompt,truncation=True, return_tensors=\"pt\").input_ids\n    input_ids = input_ids.to(device)\n\n    # original model generation\n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n    original_model_summaries.append(original_model_text_output)\n\n    # instruct model generation\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n    instruct_model_summaries.append(instruct_model_text_output)\n    \nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T18:37:24.281541Z","iopub.execute_input":"2024-11-02T18:37:24.281949Z"}},"outputs":[{"name":"stderr","text":"dialogues:  17%|‚ñà‚ñã        | 261/1500 [04:09<28:21,  1.37s/it]  Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 512). Running this sequence through the model will result in indexing errors\ndialogues:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 683/1500 [10:11<15:45,  1.16s/it]  ","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"**Compute Rouge score for both models**","metadata":{}},{"cell_type":"code","source":"original_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries,\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries,\n    use_aggregator=True,\n    use_stemmer=True,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.DataFrame({\n    'Original Model': original_model_results,\n    'Instruct Model': instruct_model_results\n}).T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T19:04:42.543237Z","iopub.execute_input":"2024-11-02T19:04:42.544133Z","iopub.status.idle":"2024-11-02T19:04:42.558495Z","shell.execute_reply.started":"2024-11-02T19:04:42.544076Z","shell.execute_reply":"2024-11-02T19:04:42.557363Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"                  rouge1    rouge2    rougeL  rougeLsum\nOriginal Model  0.200099  0.058277  0.172409   0.172571\nInstruct Model  0.222830  0.076568  0.193753   0.194324","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Original Model</th>\n      <td>0.200099</td>\n      <td>0.058277</td>\n      <td>0.172409</td>\n      <td>0.172571</td>\n    </tr>\n    <tr>\n      <th>Instruct Model</th>\n      <td>0.222830</td>\n      <td>0.076568</td>\n      <td>0.193753</td>\n      <td>0.194324</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(instruct_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T19:01:59.850555Z","iopub.execute_input":"2024-11-02T19:01:59.850973Z","iopub.status.idle":"2024-11-02T19:01:59.857009Z","shell.execute_reply.started":"2024-11-02T19:01:59.850935Z","shell.execute_reply":"2024-11-02T19:01:59.855938Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\nrouge1: 2.27%\nrouge2: 1.83%\nrougeL: 2.13%\nrougeLsum: 2.18%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"$$$$","metadata":{}},{"cell_type":"markdown","source":"## Perform Parameter Efficient Fine-Tuning ","metadata":{}},{"cell_type":"markdown","source":"<p style=\"background-color:#e6f7ff; \n          padding:15px; \n          color:#111;\n          font-size:16px;\n          border-width:3px; \n          border-color:#d0eefc; \n          border-style:solid;\n          border-radius:6px\"> üí° Now, let's perform <code>Parameter Efficient Fine-Tuning (PEFT)</code> fine-tuning as opposed to **full fine-tuning** as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results. </br> </br>\nPEFT is a generic term that includes <code>Low-Rank Adaptation (LoRA)</code> and <code>prompt tuning</code>. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources. After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged (frozen weights) and a newly-trained <code>LoRA adapter</code> emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  </br> </br>\nThat said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.</p>","metadata":{}},{"cell_type":"markdown","source":"### Setup the PEFT/LoRA model for Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"<p style=\"background-color:#e6f7ff; \n          padding:15px; \n          color:#111;\n          font-size:16px;\n          border-width:3px; \n          border-color:#d0eefc; \n          border-style:solid;\n          border-radius:6px\"> üí° Let's set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA freezes the underlying LLM weights and only training the adapter. the rank <code>r</code> of LoRA is hyper-parameter, which defines the rank/dimension of the adapter to be trained.</p>","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, TaskType\n\nlora_config = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T20:36:15.050924Z","iopub.execute_input":"2024-11-02T20:36:15.051934Z","iopub.status.idle":"2024-11-02T20:36:15.058623Z","shell.execute_reply.started":"2024-11-02T20:36:15.051845Z","shell.execute_reply":"2024-11-02T20:36:15.057364Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"from peft import get_peft_model\n\npeft_model = get_peft_model(original_model, \n                            lora_config)\npeft_model = peft_model.to(device)\nnumber_of_trainable_parameters(peft_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T20:37:08.339153Z","iopub.execute_input":"2024-11-02T20:37:08.340105Z","iopub.status.idle":"2024-11-02T20:37:08.515177Z","shell.execute_reply.started":"2024-11-02T20:37:08.340063Z","shell.execute_reply":"2024-11-02T20:37:08.514248Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 3538944\nall model parameters: 251116800\npercentage of trainable model parameters: 1.41%\n","output_type":"stream"}],"execution_count":78},{"cell_type":"markdown","source":"### Train PEFT Adapter","metadata":{}},{"cell_type":"code","source":"output_dir = f'./peft-dialogue-summary-training'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    auto_find_batch_size=True,\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=1,\n    logging_steps=150,\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    report_to='none',\n    eval_on_start=True\n)\n    \npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets['validation']\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T20:37:21.719157Z","iopub.execute_input":"2024-11-02T20:37:21.719547Z","iopub.status.idle":"2024-11-02T20:37:21.767104Z","shell.execute_reply.started":"2024-11-02T20:37:21.719508Z","shell.execute_reply":"2024-11-02T20:37:21.766112Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"peft_trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T19:47:39.619232Z","iopub.execute_input":"2024-11-02T19:47:39.620231Z","iopub.status.idle":"2024-11-02T19:47:40.489268Z","shell.execute_reply.started":"2024-11-02T19:47:39.620176Z","shell.execute_reply":"2024-11-02T19:47:40.488184Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"peft_model_path=\"./peft-dialogue-summary-checkpoint\"\npeft_trainer.model.save_pretrained(peft_model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T19:47:46.290742Z","iopub.execute_input":"2024-11-02T19:47:46.291385Z","iopub.status.idle":"2024-11-02T19:47:46.576754Z","shell.execute_reply.started":"2024-11-02T19:47:46.291345Z","shell.execute_reply":"2024-11-02T19:47:46.576026Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"**Prepare PEFT model by adding an adapter to the original FLAN-T5 model**","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\npeft_model = PeftModel.from_pretrained(peft_model_base, \n                                       peft_model_path, \n                                       is_trainable=False)\npeft_model = peft_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T19:48:50.371822Z","iopub.execute_input":"2024-11-02T19:48:50.372793Z","iopub.status.idle":"2024-11-02T19:48:52.991728Z","shell.execute_reply.started":"2024-11-02T19:48:50.372728Z","shell.execute_reply":"2024-11-02T19:48:52.990898Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"### Evaluate the Model Qualitatively (Human Evaluation)","metadata":{}},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T19:50:44.360027Z","iopub.execute_input":"2024-11-02T19:50:44.360427Z","iopub.status.idle":"2024-11-02T19:50:44.366487Z","shell.execute_reply.started":"2024-11-02T19:50:44.360387Z","shell.execute_reply":"2024-11-02T19:50:44.365435Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\ninput_ids = input_ids.to(device)\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T19:51:08.801428Z","iopub.execute_input":"2024-11-02T19:51:08.802145Z","iopub.status.idle":"2024-11-02T19:51:11.808649Z","shell.execute_reply.started":"2024-11-02T19:51:08.802090Z","shell.execute_reply":"2024-11-02T19:51:11.807710Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\n#Person1# suggests upgrading #Person2#'s system to make up some flyers and banners. #Person2# also suggests upgrading #Person1#'s hardware.\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\nYou might want to upgrade your computer.\n---------------------------------------------------------------------------------------------------\nPEFT MODEL: #Person2# wants to upgrade #Person2#'s system and hardware. #Person1# recommends adding a painting program to #Person2#'s software and adding a CD-ROM drive.\n","output_type":"stream"}],"execution_count":64},{"cell_type":"markdown","source":"### Evaluate the Model Quantitatively (with ROUGE Metric)","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test']['dialogue']\nhuman_baseline_summaries = dataset['test']['summary']\n\npeft_model_summaries = []\n\nfor dialogue in tqdm(dialogues,desc='dialogue'):\n    prompt = f\"\"\"\n        Summarize the following conversation.\n\n        {dialogue}\n\n        Summary: \"\"\"\n    \n    input_ids = tokenizer(prompt,truncation=True,\n                          return_tensors=\"pt\").input_ids\n    input_ids = input_ids.to(device)\n    \n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_summaries.append(peft_model_text_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T19:56:01.461523Z","iopub.execute_input":"2024-11-02T19:56:01.462423Z","iopub.status.idle":"2024-11-02T20:29:18.300731Z","shell.execute_reply.started":"2024-11-02T19:56:01.462380Z","shell.execute_reply":"2024-11-02T20:29:18.299768Z"}},"outputs":[{"name":"stderr","text":"dialogue: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1500/1500 [33:16<00:00,  1.33s/it]\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries,\n        columns = ['human_baseline', 'original_model', 'instruct_model', 'peft_model'])\ndf = df.sample(1)\nstyled_df = df.style.set_properties(\n    **{\n        'text-align': 'left',  \n        'white-space': 'pre-wrap',\n        'max-width': '300px', \n    }\n).set_table_attributes('style=\"width: 100%;\"')\nstyled_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T20:34:25.243562Z","iopub.execute_input":"2024-11-02T20:34:25.243946Z","iopub.status.idle":"2024-11-02T20:34:25.257345Z","shell.execute_reply.started":"2024-11-02T20:34:25.243909Z","shell.execute_reply":"2024-11-02T20:34:25.256382Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7e9273bb13f0>","text/html":"<style type=\"text/css\">\n#T_a5fca_row0_col0, #T_a5fca_row0_col1, #T_a5fca_row0_col2, #T_a5fca_row0_col3 {\n  text-align: left;\n  white-space: pre-wrap;\n  max-width: 300px;\n}\n</style>\n<table id=\"T_a5fca\" style=\"width: 100%;\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_a5fca_level0_col0\" class=\"col_heading level0 col0\" >human_baseline</th>\n      <th id=\"T_a5fca_level0_col1\" class=\"col_heading level0 col1\" >original_model</th>\n      <th id=\"T_a5fca_level0_col2\" class=\"col_heading level0 col2\" >instruct_model</th>\n      <th id=\"T_a5fca_level0_col3\" class=\"col_heading level0 col3\" >peft_model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_a5fca_level0_row0\" class=\"row_heading level0 row0\" >1307</th>\n      <td id=\"T_a5fca_row0_col0\" class=\"data row0 col0\" >Greg Sonders calls Mary to ask whether Mary is interested in sports and tells Mary to wait for final admission decision later.</td>\n      <td id=\"T_a5fca_row0_col1\" class=\"data row0 col1\" >Mr. Sonders, please tell me a little bit about yourself.</td>\n      <td id=\"T_a5fca_row0_col2\" class=\"data row0 col2\" >Greg Sonders from Brown College is speaking to Mary.</td>\n      <td id=\"T_a5fca_row0_col3\" class=\"data row0 col3\" >Greg Sonders is speaking to Mary. He asks Mary if she'd be interested in college sports. Mary tells Greg Sonders she plays volleyball and she's impressed.</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":70},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries,\n    use_aggregator=True,\n    use_stemmer=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T20:34:52.352835Z","iopub.execute_input":"2024-11-02T20:34:52.353693Z","iopub.status.idle":"2024-11-02T20:34:57.941190Z","shell.execute_reply.started":"2024-11-02T20:34:52.353653Z","shell.execute_reply":"2024-11-02T20:34:57.940088Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"pd.DataFrame({\n    'Original Model': original_model_results,\n    'Instruct Model': instruct_model_results,\n    'PEFT Model': peft_model_results\n}).T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T20:35:24.622520Z","iopub.execute_input":"2024-11-02T20:35:24.623132Z","iopub.status.idle":"2024-11-02T20:35:24.634929Z","shell.execute_reply.started":"2024-11-02T20:35:24.623093Z","shell.execute_reply":"2024-11-02T20:35:24.633891Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"                  rouge1    rouge2    rougeL  rougeLsum\nOriginal Model  0.200099  0.058277  0.172409   0.172571\nInstruct Model  0.222830  0.076568  0.193753   0.194324\nPEFT Model      0.403744  0.154918  0.321592   0.321941","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Original Model</th>\n      <td>0.200099</td>\n      <td>0.058277</td>\n      <td>0.172409</td>\n      <td>0.172571</td>\n    </tr>\n    <tr>\n      <th>Instruct Model</th>\n      <td>0.222830</td>\n      <td>0.076568</td>\n      <td>0.193753</td>\n      <td>0.194324</td>\n    </tr>\n    <tr>\n      <th>PEFT Model</th>\n      <td>0.403744</td>\n      <td>0.154918</td>\n      <td>0.321592</td>\n      <td>0.321941</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":72},{"cell_type":"markdown","source":"**Improvement of PEFT model over original model**","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T20:35:36.375987Z","iopub.execute_input":"2024-11-02T20:35:36.376710Z","iopub.status.idle":"2024-11-02T20:35:36.382519Z","shell.execute_reply.started":"2024-11-02T20:35:36.376665Z","shell.execute_reply":"2024-11-02T20:35:36.381626Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 20.36%\nrouge2: 9.66%\nrougeL: 14.92%\nrougeLsum: 14.94%\n","output_type":"stream"}],"execution_count":73},{"cell_type":"markdown","source":"**Improvement of PEFT model over Instruct model**","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-02T20:35:48.386068Z","iopub.execute_input":"2024-11-02T20:35:48.386421Z","iopub.status.idle":"2024-11-02T20:35:48.392649Z","shell.execute_reply.started":"2024-11-02T20:35:48.386388Z","shell.execute_reply":"2024-11-02T20:35:48.391679Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\nrouge1: 18.09%\nrouge2: 7.83%\nrougeL: 12.78%\nrougeLsum: 12.76%\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}